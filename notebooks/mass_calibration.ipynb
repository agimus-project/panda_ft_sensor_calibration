{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pinocchio as pin\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "# Set numpy precision to show easier to understand values\n",
    "np.set_printoptions(formatter={\"float_kind\": \"{:.3f}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the paths with your own data\n",
    "\n",
    "df_joint_states = pd.read_parquet(\"data/jtc/franka_joint_states.parquet\")\n",
    "df_ft = pd.read_parquet(\"data/jtc/force_torque_sensor_broadcaster_wrench.parquet\")\n",
    "df_moving_to_pose = pd.read_parquet(\"data/jtc/moving_to_new_pose.parquet\")\n",
    "df_pose_reached = pd.read_parquet(\"data/jtc/pose_reached.parquet\")\n",
    "\n",
    "df_robot_description = pd.read_parquet(\"data/jtc/robot_description.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract timestamps of stationary poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables to easier detect state transitions after concatenations\n",
    "df_pose_reached[\"state\"] = 0\n",
    "df_moving_to_pose[\"state\"] = 1\n",
    "# Filter moving to pose topics that are after a first pose was reached\n",
    "df_moving_to_pose = df_moving_to_pose.loc[\n",
    "    (df_moving_to_pose[\"timestamp\"] - df_pose_reached[\"timestamp\"][0]) > 0\n",
    "]\n",
    "# Concatenate both dataframes and sort new dataframe by timestamp\n",
    "df_stamps = pd.concat((df_moving_to_pose, df_pose_reached)).sort_values(\n",
    "    by=[\"timestamp\"], ignore_index=True\n",
    ")\n",
    "# Filter timestamps where robot stays in place. This means difference between next state and current state is -1\n",
    "# And time difference between messages appearing has to be larder than 19 second (robot was suppose to wait 20 seconds)\n",
    "df_stamps_stationary = df_stamps[\n",
    "    (df_stamps[\"state\"].diff() == -1.0) & (-df_stamps[\"timestamp\"].diff(-1) > 19 * 1e9)\n",
    "]\n",
    "df_stamps_stationary.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create more accurate timestamps based on header messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joint_states[\"header.stamp\"] = (\n",
    "    df_joint_states[\"header.stamp.sec\"] * 1e9 + df_joint_states[\"header.stamp.nanosec\"]\n",
    ")\n",
    "df_ft[\"header.stamp\"] = df_ft[\"header.stamp.sec\"] * 1e9 + df_ft[\"header.stamp.nanosec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract parts of the dataframes that correspond only to the measured poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dataframes to topic names\n",
    "measurements = {\n",
    "    \"joint_states\": df_joint_states,\n",
    "    \"force_torque\": df_ft,\n",
    "}\n",
    "\n",
    "stationary_data = {}\n",
    "# Choose offsets for clipping\n",
    "start_stamp_offset = int(3.0 * 1e9)  # 3 seconds\n",
    "end_stamp_offset = int(19.0 * 1e9)  # 19 seconds\n",
    "for i, data in df_stamps_stationary.iterrows():\n",
    "    # Compute start and end time\n",
    "    start = data[\"timestamp\"] + start_stamp_offset\n",
    "    end = data[\"timestamp\"] + end_stamp_offset\n",
    "    pose_name = data[\"data\"]\n",
    "    if pose_name not in stationary_data.keys():\n",
    "        stationary_data[pose_name] = []\n",
    "    stationary_data[pose_name].append(\n",
    "        {\n",
    "            topic: df.loc[(df[\"header.stamp\"] > start) & (df[\"header.stamp\"] < end)]\n",
    "            for topic, df in measurements.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load URDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = pin.buildModelFromXML(df_robot_description[\"data\"][0])\n",
    "joints_to_lock = [\n",
    "    model_full.getJointId(name) for name in model_full.names if \"finger\" in name\n",
    "]\n",
    "model = pin.buildReducedModel(model_full, joints_to_lock, np.zeros(model_full.nq))\n",
    "\n",
    "data = model.createData()\n",
    "\n",
    "joint_order = [name for name in model.names if name != \"universe\"]\n",
    "base_frame_id = model.getFrameId(\"fer_link0\")\n",
    "tcp_pose_frame_id = model.getFrameId(\"fer_link8\")\n",
    "sensor_frame_id = model.getFrameId(\"ati_mini45_measurement_reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap joint order\n",
    "Joint state publisher does not guarantee to be publish joints in the same order all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that will not be used\n",
    "columns_to_drop = [\n",
    "    \"timestamp\",\n",
    "    \"header.stamp.sec\",\n",
    "    \"header.stamp.nanosec\",\n",
    "    \"header.frame_id\",\n",
    "]\n",
    "\n",
    "\n",
    "def reorder_joints(row):\n",
    "    joints = row[\"name\"]\n",
    "    joint_map = [np.argwhere(joints == joint_name)[0][0] for joint_name in joint_order]\n",
    "    row[\"name\"] = joint_order\n",
    "    row[\"position\"] = row[\"position\"][joint_map]\n",
    "    row[\"velocity\"] = row[\"velocity\"][joint_map]\n",
    "    row[\"effort\"] = row[\"effort\"][joint_map]\n",
    "    return row\n",
    "\n",
    "\n",
    "for pose in stationary_data.keys():\n",
    "    for i in range(len(stationary_data[pose])):\n",
    "        df1 = stationary_data[pose][i][\"force_torque\"].drop(columns=columns_to_drop)\n",
    "        df2 = stationary_data[pose][i][\"joint_states\"].drop(columns=columns_to_drop)\n",
    "        df_merged = pd.merge(df1, df2, on=[\"header.stamp\"]).dropna()\n",
    "        stationary_data[pose][i][\"merged\"] = df_merged.apply(reorder_joints, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute average values for each configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51872/1516012944.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  stationary_data[pose][i][\"merged\"].drop(columns=[\"name\", \"position\"]).mean()\n"
     ]
    }
   ],
   "source": [
    "averaged_stationary_data = {}\n",
    "\n",
    "for pose in stationary_data.keys():\n",
    "    averaged_stationary_data[pose] = []\n",
    "    for i in range(len(stationary_data[pose])):\n",
    "        position = stationary_data[pose][i][\"merged\"][\"position\"]\n",
    "        averaged_stationary_data[pose].append(\n",
    "            stationary_data[pose][i][\"merged\"].drop(columns=[\"name\", \"position\"]).mean()\n",
    "        )\n",
    "        # Position has to be computed separately as it is not a scalar\n",
    "        averaged_stationary_data[pose][i][\"position\"] = position.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute rotation matrices and force and torque vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_stationary_data = {}\n",
    "\n",
    "for pose in averaged_stationary_data.keys():\n",
    "    processed_stationary_data[pose] = []\n",
    "    for i in range(len(averaged_stationary_data[pose])):\n",
    "        data_sample = averaged_stationary_data[pose][i]\n",
    "        pin.framesForwardKinematics(model, data, data_sample[\"position\"])\n",
    "\n",
    "        R_wm = data.oMf[base_frame_id].actInv(data.oMf[tcp_pose_frame_id]).rotation\n",
    "        R_ws = data.oMf[base_frame_id].actInv(data.oMf[sensor_frame_id]).rotation\n",
    "        processed_stationary_data[pose].append(\n",
    "            {\n",
    "                # Rotation from world to mounting plate\n",
    "                \"R_wm\": np.array(R_wm, copy=True),\n",
    "                # Rotation from world to sensor frame\n",
    "                \"R_ws\": np.array(R_ws, copy=True),\n",
    "                # Negate both force and torque as measurements are flipped\n",
    "                \"f\": -np.array([data_sample[\"wrench.force.\" + dir] for dir in \"xyz\"]),\n",
    "                \"tau\": -np.array(\n",
    "                    [data_sample[\"wrench.torque.\" + dir] for dir in \"xyz\"]\n",
    "                ),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate rotations\n",
    "\n",
    "Ensure rotation matrices computed from averaged joint configurations match valued expected during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose name       delta angle in degrees\n",
      "\n",
      "down_1          [-0.093 -0.016 0.567]\n",
      "down_0          [-0.090 -0.017 0.384]\n",
      "down_2          [-0.074 -0.018 0.238]\n",
      "up_side_down_1  [-0.155 -0.588 0.536]\n",
      "up_side_down_2  [-0.240 -0.560 0.116]\n",
      "up_side_down_0  [-0.095 -0.486 0.190]\n",
      "curled_1        [0.266 0.467 -0.016]\n",
      "curled_0        [0.097 0.459 -0.033]\n",
      "curled_2        [0.098 0.434 -0.033]\n",
      "up_right_0      [-0.277 -0.267 0.306]\n",
      "up_right_1      [-0.224 -0.015 0.269]\n",
      "up_right_2      [-0.172 -0.075 0.230]\n",
      "up_right_3      [-0.124 -0.052 0.203]\n",
      "stretched_0     [0.692 0.305 0.140]\n",
      "stretched_1     [0.283 0.433 0.031]\n",
      "stretched_2     [0.337 0.400 -0.025]\n",
      "stretched_3     [0.145 0.355 -0.206]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config/reference_pose_publisher_params.yaml\") as stream:\n",
    "    try:\n",
    "        params = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "params = params[\"/**\"][\"ros__parameters\"]\n",
    "keys = [key for key in set(params[\"poses_to_reach_names\"]) if \"_transition\" not in key]\n",
    "expected_configurations = {\n",
    "    key: pin.Quaternion(np.array(params[key][\"quat\"])) for key in keys\n",
    "}\n",
    "\n",
    "print(f\"{'pose name':15} delta angle in degrees\")\n",
    "print()\n",
    "for p in processed_stationary_data.keys():\n",
    "    pose = processed_stationary_data[p][0]\n",
    "    expected_quat = expected_configurations[p]\n",
    "\n",
    "    diff = expected_quat * pin.Quaternion(pose[\"R_wm\"].T)\n",
    "    print(f\"{p:15}\", np.rad2deg(pin.rpy.matrixToRpy(diff.matrix())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model only on forces\n",
    "\n",
    "This checks if a simpler model can be fit. Initial mass is larger than what can be found in URDF of the robot. Rotation is between expected frame, based on the datasheet making the whole method do only small refinement, smaller than 5 degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 0.7881260028387547 [kg]\n",
      "v: [-1.040 0.089 -0.073] [deg]\n",
      "b: [0.121 -0.269 7.788] [N]\n"
     ]
    }
   ],
   "source": [
    "# Create gravity vector\n",
    "g = np.array([0.0, 0.0, -9.81])\n",
    "\n",
    "# Initial guess for bias based on two poses, where expected measured force can be guesses\n",
    "b_init = (\n",
    "    processed_stationary_data[\"down_0\"][0][\"f\"]\n",
    "    + processed_stationary_data[\"up_side_down_0\"][0][\"f\"]\n",
    ") / 2\n",
    "# Bound expected bias\n",
    "b_lim = [sorted((0.5 * b, 1.5 * b)) for b in b_init]\n",
    "\n",
    "\n",
    "# Cost function\n",
    "def cost(x):\n",
    "    # Mass\n",
    "    m = x[0]\n",
    "    # Angular velocity between expected sensor frame and refined sensor frame\n",
    "    v = x[1:4]\n",
    "    # Bias\n",
    "    b = x[4:]\n",
    "    sum = 0.0\n",
    "    j = 0.0\n",
    "    for p in processed_stationary_data.keys():\n",
    "        for i in range(len(processed_stationary_data[p])):\n",
    "            pose = processed_stationary_data[p][i]\n",
    "            R = pose[\"R_ws\"].copy()\n",
    "            f = pose[\"f\"].copy()\n",
    "\n",
    "            diff = m * pin.exp3(v).T @ R.T @ g - (f - b)\n",
    "            sum += np.sum(diff * diff)\n",
    "            j += 1.0\n",
    "    # Return normalized cost\n",
    "    return sum / j\n",
    "\n",
    "\n",
    "rot_err = np.deg2rad(5)\n",
    "\n",
    "# SLSQP is used without gradient.\n",
    "# This way it approximates it.\n",
    "res = scipy.optimize.minimize(\n",
    "    cost,\n",
    "    [0.8, 0.0, 0.0, 0.0, *b_init],\n",
    "    method=\"SLSQP\",\n",
    "    bounds=[\n",
    "        (0.78, 1.0),\n",
    "        (-rot_err, rot_err),\n",
    "        (-rot_err, rot_err),\n",
    "        (-rot_err, rot_err),\n",
    "        *b_lim,\n",
    "    ],\n",
    "    tol=1e-7,\n",
    ")\n",
    "m = res.x[0]\n",
    "v = res.x[1:4]\n",
    "b = res.x[4:]\n",
    "print(f\"m: {m} [kg]\")\n",
    "print(f\"v: {np.rad2deg(pin.rpy.matrixToRpy(pin.exp3(v)))} [deg]\")\n",
    "print(f\"b: {b} [N]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate results\n",
    "\n",
    "Convert unbiased, measured force to world frame. Expected values is, all vectors are close to gravity vector and cost being close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose name       Normalized measured force in world frame   cost\n",
      "\n",
      "down_1          [0.024 -0.231 -9.941]                      0.044\n",
      "down_0          [0.181 -0.281 -9.945]                      0.081\n",
      "down_2          [0.270 -0.173 -9.959]                      0.077\n",
      "up_side_down_1  [-0.163 -0.277 -10.277]                    0.199\n",
      "up_side_down_2  [-0.022 -0.157 -10.237]                    0.129\n",
      "up_side_down_0  [-0.075 -0.137 -10.240]                    0.130\n",
      "curled_1        [0.382 -0.255 -9.462]                      0.206\n",
      "curled_0        [0.221 -0.034 -9.715]                      0.037\n",
      "curled_2        [0.153 -0.334 -9.650]                      0.100\n",
      "up_right_0      [-0.248 0.063 -9.750]                      0.043\n",
      "up_right_1      [0.064 -0.082 -9.471]                      0.078\n",
      "up_right_2      [-0.101 -0.209 -9.673]                     0.045\n",
      "up_right_3      [0.217 -0.244 -9.654]                      0.082\n",
      "stretched_0     [-0.120 -0.401 -9.791]                     0.109\n",
      "stretched_1     [0.053 -0.087 -9.479]                      0.074\n",
      "stretched_2     [0.176 -0.225 -9.645]                      0.068\n",
      "stretched_3     [0.182 0.174 -9.751]                       0.042\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'pose name':15} {'Normalized measured force in world frame':42} cost\")\n",
    "print()\n",
    "for p in processed_stationary_data.keys():\n",
    "    pose = processed_stationary_data[p][0]\n",
    "    R = pose[\"R_ws\"].copy()\n",
    "    f = pose[\"f\"].copy()\n",
    "    diff = m * pin.exp3(v).T @ R.T @ g - (f - b)\n",
    "    cost = np.sum(diff * diff)\n",
    "    g_reconstruct = R @ pin.exp3(v) @ (f - b) / m\n",
    "    print(f\"{p:15} {str(g_reconstruct):42} {cost:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model with forces and torques\n",
    "\n",
    "Extend the model to expect both forces and torques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 0.7881268685939541 [kg]\n",
      "vr: [-1.040 0.089 -0.073] [deg]\n",
      "vm: [0.004 0.006 -0.050] [m]\n",
      "b: [0.121 -0.269 7.788 0.048 -0.037 -0.003] [N, N/m]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/optimize/_optimize.py:284: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  warnings.warn(\"Values in x were outside bounds during a \"\n"
     ]
    }
   ],
   "source": [
    "# Create gravity vector\n",
    "g = np.array([0.0, 0.0, -9.81])\n",
    "\n",
    "# Initial guess with force and torque\n",
    "b_init = (\n",
    "    np.hstack([processed_stationary_data[\"down_0\"][0][val] for val in (\"f\", \"tau\")])\n",
    "    + np.hstack(\n",
    "        [processed_stationary_data[\"up_side_down_0\"][0][val] for val in (\"f\", \"tau\")]\n",
    "    )\n",
    ") / 2\n",
    "# Create bounds for the initial guess\n",
    "b_lim = [sorted((0.5 * b, 1.5 * b)) for b in b_init]\n",
    "\n",
    "\n",
    "def cost(x):\n",
    "    # Mass\n",
    "    m = x[0]\n",
    "    # Angular velocity\n",
    "    vr = np.array(x[1:4])\n",
    "    # Angular velocity between expected sensor frame and refined sensor frame\n",
    "    vm = np.array(x[4:7])\n",
    "    # Bias\n",
    "    b = np.array(x[7:])\n",
    "    sum = 0.0\n",
    "    j = 0.0\n",
    "    for p in processed_stationary_data.keys():\n",
    "        for i in range(len(processed_stationary_data[p])):\n",
    "            pose = processed_stationary_data[p][i]\n",
    "            R = pose[\"R_ws\"].copy()\n",
    "            f = pose[\"f\"].copy()\n",
    "            tau = pose[\"tau\"].copy()\n",
    "\n",
    "            diff = np.vstack((np.eye(3), pin.skew(vm))) @ (\n",
    "                m * pin.exp3(vr).T @ R.T @ g\n",
    "            ) - (np.hstack((f, tau)) - b)\n",
    "            sum += np.sum(diff * diff)\n",
    "            j += 1.0\n",
    "    # Return normalized cost\n",
    "    return sum / j\n",
    "\n",
    "\n",
    "rot_err = np.deg2rad(5)\n",
    "\n",
    "res = scipy.optimize.minimize(\n",
    "    cost,\n",
    "    [0.8, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, *b_init],\n",
    "    method=\"SLSQP\",\n",
    "    bounds=[\n",
    "        (0.78, 1.0),\n",
    "        (-rot_err, rot_err),\n",
    "        (-rot_err, rot_err),\n",
    "        (-rot_err, rot_err),\n",
    "        (-0.05, 0.05),\n",
    "        (-0.05, 0.05),\n",
    "        (-0.2, 0.2),\n",
    "        *b_lim,\n",
    "    ],\n",
    "    tol=1e-7,\n",
    ")\n",
    "m = res.x[0]\n",
    "vr = np.array(res.x[1:4])\n",
    "vm = np.array(res.x[4:7])\n",
    "b = np.array(res.x[7:])\n",
    "\n",
    "print(f\"m: {m} [kg]\")\n",
    "print(f\"vr: {np.rad2deg(pin.rpy.matrixToRpy(pin.exp3(v)))} [deg]\")\n",
    "print(f\"vm: {vm} [m]\")\n",
    "print(f\"b: {b} [N, N/m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate results\n",
    "\n",
    "Check if torques with bias removed can be recreated from measured forces without the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose name       Measured torque        Computed torque       \n",
      "\n",
      "down_1          [-0.050 0.038 0.004]   [-0.039 0.038 0.002]  \n",
      "down_1          [-0.048 0.036 0.004]   [-0.044 0.031 0.001]  \n",
      "down_0          [-0.040 0.030 0.004]   [-0.029 0.029 0.002]  \n",
      "down_0          [-0.038 0.031 0.003]   [-0.034 0.026 0.001]  \n",
      "down_2          [-0.047 0.018 0.004]   [-0.037 0.017 -0.001] \n",
      "down_2          [-0.039 0.015 0.004]   [-0.039 0.012 -0.001] \n",
      "up_side_down_1  [0.035 -0.034 -0.004]  [0.040 -0.041 -0.002] \n",
      "up_side_down_1  [0.031 -0.033 -0.002]  [0.036 -0.046 -0.003] \n",
      "up_side_down_2  [0.050 -0.020 0.001]   [0.051 -0.024 0.001]  \n",
      "up_side_down_2  [0.050 -0.018 -0.001]  [0.046 -0.028 -0.000] \n",
      "up_side_down_0  [0.040 -0.023 -0.009]  [0.039 -0.026 -0.000] \n",
      "up_side_down_0  [0.035 -0.022 -0.003]  [0.032 -0.032 -0.002] \n",
      "curled_1        [-0.329 0.194 -0.001]  [-0.319 0.193 0.001]  \n",
      "curled_1        [-0.324 0.195 -0.001]  [-0.323 0.190 0.000]  \n",
      "curled_0        [0.193 0.340 0.061]    [0.193 0.332 0.057]   \n",
      "curled_0        [0.196 0.339 0.059]    [0.188 0.326 0.056]   \n",
      "curled_2        [0.332 -0.201 0.000]   [0.325 -0.202 -0.002] \n",
      "curled_2        [0.336 -0.202 -0.001]  [0.320 -0.207 -0.003] \n",
      "up_right_0      [-0.198 -0.338 -0.058] [-0.185 -0.337 -0.057]\n",
      "up_right_0      [-0.200 -0.338 -0.054] [-0.189 -0.347 -0.058]\n",
      "up_right_1      [-0.330 0.191 0.001]   [-0.324 0.190 0.000]  \n",
      "up_right_1      [-0.329 0.194 -0.003]  [-0.329 0.185 -0.001] \n",
      "up_right_2      [0.189 0.342 0.062]    [0.187 0.333 0.056]   \n",
      "up_right_2      [0.192 0.341 0.060]    [0.182 0.327 0.055]   \n",
      "up_right_3      [0.333 -0.195 0.001]   [0.325 -0.198 -0.001] \n",
      "up_right_3      [0.336 -0.196 -0.001]  [0.320 -0.204 -0.002] \n",
      "stretched_0     [-0.194 -0.339 -0.056] [-0.183 -0.341 -0.057]\n",
      "stretched_0     [-0.190 -0.343 -0.055] [-0.183 -0.351 -0.058]\n",
      "stretched_1     [-0.330 0.188 -0.000]  [-0.326 0.186 -0.000] \n",
      "stretched_1     [-0.328 0.186 -0.001]  [-0.333 0.178 -0.002] \n",
      "stretched_2     [0.187 0.345 0.061]    [0.184 0.334 0.056]   \n",
      "stretched_2     [0.188 0.344 0.060]    [0.178 0.329 0.055]   \n",
      "stretched_3     [0.337 -0.192 0.002]   [0.330 -0.199 -0.001] \n",
      "stretched_3     [0.341 -0.193 0.001]   [0.325 -0.205 -0.002] \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'pose name':15} {'Measured torque':22} {'Computed torque':22}\")\n",
    "print()\n",
    "for p in processed_stationary_data.keys():\n",
    "    for i in range(len(processed_stationary_data[p])):\n",
    "        pose = processed_stationary_data[p][i]\n",
    "        R = pose[\"R_ws\"].copy()\n",
    "        f = pose[\"f\"].copy()\n",
    "        tau = pose[\"tau\"].copy()\n",
    "\n",
    "        print(f\"{p:15} {str(tau - b[3:]):22} {str(np.cross(vm, f - b[:3])):22}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic calibration. TBD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/control_measurements.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_control \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/control_measurements.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_ft \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/force_torque_sensor_broadcaster_wrench_measurements.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m df_franka_joint_states \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/franka_joint_states_measurements.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parquet.py:495\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parquet.py:232\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    230\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    240\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    241\u001b[0m     )\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parquet.py:101\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m     91\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/common.py:711\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    702\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    703\u001b[0m             handle,\n\u001b[1;32m    704\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    707\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    708\u001b[0m         )\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/control_measurements.parquet'"
     ]
    }
   ],
   "source": [
    "df_control = pd.read_parquet(\"results/control_measurements.parquet\")\n",
    "df_ft = pd.read_parquet(\n",
    "    \"results/force_torque_sensor_broadcaster_wrench_measurements.parquet\"\n",
    ")\n",
    "df_franka_joint_states = pd.read_parquet(\n",
    "    \"results/franka_joint_states_measurements.parquet\"\n",
    ")\n",
    "df_sensor = pd.read_parquet(\"results/sensor_measurements.parquet\")\n",
    "\n",
    "\n",
    "df_joint_states[\"header.stamp\"] = (\n",
    "    df_joint_states[\"header.stamp.sec\"] * 1e9 + df_joint_states[\"header.stamp.nanosec\"]\n",
    ")\n",
    "df_ft[\"header.stamp\"] = df_ft[\"header.stamp.sec\"] * 1e9 + df_ft[\"header.stamp.nanosec\"]\n",
    "\n",
    "# Map dataframes to topic names\n",
    "measurements = {\n",
    "    \"joint_states\": df_joint_states,\n",
    "    \"force_torque\": df_ft,\n",
    "}\n",
    "\n",
    "stationary_data = {}\n",
    "# Choose offsets for clipping\n",
    "start_stamp_offset = int(3.0 * 1e9)  # 3 seconds\n",
    "end_stamp_offset = int(19.0 * 1e9)  # 19 seconds\n",
    "for i, data in df_stamps_stationary.iterrows():\n",
    "    # Compute start and end time\n",
    "    start = data[\"timestamp\"] + start_stamp_offset\n",
    "    end = data[\"timestamp\"] + end_stamp_offset\n",
    "    pose_name = data[\"data\"]\n",
    "    if pose_name not in stationary_data.keys():\n",
    "        stationary_data[pose_name] = []\n",
    "    stationary_data[pose_name].append(\n",
    "        {\n",
    "            topic: df.loc[(df[\"header.stamp\"] > start) & (df[\"header.stamp\"] < end)]\n",
    "            for topic, df in measurements.items()\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Remove columns that will not be used\n",
    "columns_to_drop = [\n",
    "    \"timestamp\",\n",
    "    \"header.stamp.sec\",\n",
    "    \"header.stamp.nanosec\",\n",
    "    \"header.frame_id\",\n",
    "]\n",
    "\n",
    "\n",
    "def reorder_joints(row):\n",
    "    joints = row[\"name\"]\n",
    "    joint_map = [np.argwhere(joints == joint_name)[0][0] for joint_name in joint_order]\n",
    "    row[\"name\"] = joint_order\n",
    "    row[\"position\"] = row[\"position\"][joint_map]\n",
    "    row[\"velocity\"] = row[\"velocity\"][joint_map]\n",
    "    row[\"effort\"] = row[\"effort\"][joint_map]\n",
    "    return row\n",
    "\n",
    "\n",
    "for pose in stationary_data.keys():\n",
    "    for i in range(len(stationary_data[pose])):\n",
    "        df1 = stationary_data[pose][i][\"force_torque\"].drop(columns=columns_to_drop)\n",
    "        df2 = stationary_data[pose][i][\"joint_states\"].drop(columns=columns_to_drop)\n",
    "        df_merged = pd.merge(df1, df2, on=[\"header.stamp\"]).dropna()\n",
    "        stationary_data[pose][i][\"merged\"] = df_merged.apply(reorder_joints, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
